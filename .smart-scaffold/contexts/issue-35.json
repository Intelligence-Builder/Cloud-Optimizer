{
  "issue_number": 35,
  "title": "8.1 NLU Pipeline",
  "body": "# 8.1 NLU Pipeline\n\n## Parent Epic\nEpic 8: MVP Phase 2 - Expert System (Intelligence-Builder)\n\n## Overview\n\nImplement the Natural Language Understanding pipeline that processes user questions to extract intent, entities, and context for answer generation. Uses LLM-based classification for flexibility and accuracy.\n\n## Background\n\nThe NLU pipeline is the entry point for all user questions. It must:\n- Classify question intent (security advice, finding explanation, compliance question)\n- Extract entities (AWS services, compliance frameworks, resources)\n- Build context from conversation history\n- Handle multi-turn conversations\n\n## Requirements\n\n| ID | Requirement | Acceptance Criteria |\n|----|-------------|---------------------|\n| NLU-001 | Intent classification | Classify into security_advice, finding_explanation, compliance_question, document_analysis, general_question |\n| NLU-002 | Entity extraction | Extract AWS services, compliance frameworks, resources, severity levels |\n| NLU-003 | Context tracking | Track conversation history, reference previous messages |\n| NLU-004 | Question analysis | Detect question type, extract key topics, identify required context |\n\n## Technical Specification\n\n### Intent Classification\n\n```python\n# src/ib_platform/nlu/intents.py\nfrom enum import Enum\n\nclass Intent(Enum):\n    SECURITY_ADVICE = \"security_advice\"          # General security questions\n    FINDING_EXPLANATION = \"finding_explanation\"  # Explain a specific finding\n    COMPLIANCE_QUESTION = \"compliance_question\"  # Compliance-related questions\n    DOCUMENT_ANALYSIS = \"document_analysis\"      # Questions about uploaded docs\n    COST_OPTIMIZATION = \"cost_optimization\"      # Cost-related questions\n    REMEDIATION_HELP = \"remediation_help\"        # How to fix something\n    GENERAL_QUESTION = \"general_question\"        # General cloud/security questions\n    GREETING = \"greeting\"                        # Greetings\n    OUT_OF_SCOPE = \"out_of_scope\"                # Not cloud security related\n\n\nINTENT_EXAMPLES = {\n    Intent.SECURITY_ADVICE: [\n        \"What security concerns should I have with S3?\",\n        \"Is my Redshift cluster secure?\",\n        \"How should I secure my API Gateway?\",\n    ],\n    Intent.FINDING_EXPLANATION: [\n        \"Explain this finding about public S3 buckets\",\n        \"What does the IAM_002 finding mean?\",\n        \"Why is this marked as critical?\",\n    ],\n    Intent.COMPLIANCE_QUESTION: [\n        \"What do I need for HIPAA compliance?\",\n        \"How does this relate to SOC 2?\",\n        \"Is this a PCI-DSS violation?\",\n    ],\n    Intent.DOCUMENT_ANALYSIS: [\n        \"What security issues do you see in my architecture?\",\n        \"Review this diagram for vulnerabilities\",\n        \"What's missing from my design?\",\n    ],\n    Intent.COST_OPTIMIZATION: [\n        \"How can I reduce my AWS costs?\",\n        \"Are there unused resources?\",\n        \"Should I use reserved instances?\",\n    ],\n    Intent.REMEDIATION_HELP: [\n        \"How do I fix this S3 bucket issue?\",\n        \"Show me the Terraform to fix this\",\n        \"What's the remediation for this?\",\n    ],\n}\n```\n\n### Entity Extraction\n\n```python\n# src/ib_platform/nlu/entities.py\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass ExtractedEntities:\n    aws_services: list[str]           # [\"S3\", \"RDS\", \"Lambda\"]\n    compliance_frameworks: list[str]  # [\"HIPAA\", \"SOC2\"]\n    resource_ids: list[str]           # [\"bucket-name\", \"i-abc123\"]\n    severity_levels: list[str]        # [\"critical\", \"high\"]\n    finding_ids: list[str]            # [\"S3_001\", \"IAM_002\"]\n    regions: list[str]                # [\"us-east-1\"]\n    keywords: list[str]               # Key terms extracted\n\n\nAWS_SERVICES = {\n    \"s3\": [\"s3\", \"bucket\", \"buckets\", \"simple storage\"],\n    \"ec2\": [\"ec2\", \"instance\", \"instances\", \"virtual machine\", \"vm\"],\n    \"rds\": [\"rds\", \"database\", \"databases\", \"aurora\", \"mysql\", \"postgres\"],\n    \"iam\": [\"iam\", \"user\", \"users\", \"role\", \"roles\", \"policy\", \"permission\"],\n    \"lambda\": [\"lambda\", \"function\", \"serverless\"],\n    \"vpc\": [\"vpc\", \"network\", \"subnet\", \"security group\"],\n    \"kms\": [\"kms\", \"key\", \"encryption\"],\n    \"cloudtrail\": [\"cloudtrail\", \"trail\", \"logging\", \"audit\"],\n    \"redshift\": [\"redshift\", \"warehouse\", \"data warehouse\"],\n    \"glue\": [\"glue\", \"etl\", \"catalog\"],\n}\n\nCOMPLIANCE_FRAMEWORKS = {\n    \"hipaa\": [\"hipaa\", \"phi\", \"healthcare\", \"patient\"],\n    \"soc2\": [\"soc2\", \"soc 2\", \"type 2\", \"type ii\"],\n    \"pci\": [\"pci\", \"pci-dss\", \"payment\", \"cardholder\"],\n    \"gdpr\": [\"gdpr\", \"privacy\", \"data protection\", \"eu\"],\n    \"cis\": [\"cis\", \"benchmark\", \"benchmarks\"],\n    \"nist\": [\"nist\", \"800-53\", \"framework\"],\n}\n```\n\n### NLU Service\n\n```python\n# src/ib_platform/nlu/service.py\nfrom anthropic import Anthropic\n\nclass NLUService:\n    def __init__(self, anthropic_client: Anthropic):\n        self.client = anthropic_client\n\n    async def process(self, message: str, conversation_history: list[dict]) -> NLUResult:\n        \"\"\"Process user message through NLU pipeline.\"\"\"\n        # Run intent classification and entity extraction in parallel\n        intent_task = asyncio.create_task(self._classify_intent(message))\n        entities_task = asyncio.create_task(self._extract_entities(message))\n\n        intent, confidence = await intent_task\n        entities = await entities_task\n\n        # Build context from conversation history\n        context = self._build_context(conversation_history, entities)\n\n        return NLUResult(\n            intent=intent,\n            confidence=confidence,\n            entities=entities,\n            context=context,\n            requires_findings=self._requires_findings(intent),\n            requires_documents=self._requires_documents(intent, conversation_history),\n        )\n\n    async def _classify_intent(self, message: str) -> tuple[Intent, float]:\n        \"\"\"Classify message intent using Claude.\"\"\"\n        prompt = f\"\"\"Classify the following user message into one of these intents:\n- security_advice: General security questions about AWS services\n- finding_explanation: Questions about specific scan findings\n- compliance_question: Questions about compliance frameworks (HIPAA, SOC2, etc.)\n- document_analysis: Questions about uploaded architecture documents\n- cost_optimization: Questions about reducing costs\n- remediation_help: Questions about how to fix security issues\n- general_question: General cloud questions\n- greeting: Greetings or small talk\n- out_of_scope: Not related to cloud security\n\nMessage: \"{message}\"\n\nRespond with JSON: {{\"intent\": \"intent_name\", \"confidence\": 0.0-1.0}}\"\"\"\n\n        response = await self.client.messages.create(\n            model=\"claude-3-haiku-20240307\",\n            max_tokens=100,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n\n        result = json.loads(response.content[0].text)\n        return Intent(result[\"intent\"]), result[\"confidence\"]\n\n    async def _extract_entities(self, message: str) -> ExtractedEntities:\n        \"\"\"Extract entities from message.\"\"\"\n        message_lower = message.lower()\n\n        # Extract AWS services\n        aws_services = []\n        for service, keywords in AWS_SERVICES.items():\n            if any(kw in message_lower for kw in keywords):\n                aws_services.append(service.upper())\n\n        # Extract compliance frameworks\n        compliance = []\n        for framework, keywords in COMPLIANCE_FRAMEWORKS.items():\n            if any(kw in message_lower for kw in keywords):\n                compliance.append(framework.upper())\n\n        # Extract finding IDs (pattern: XXX_NNN)\n        finding_ids = re.findall(r'\\b([A-Z]{2,}_\\d{3})\\b', message)\n\n        # Extract severity levels\n        severities = []\n        for sev in [\"critical\", \"high\", \"medium\", \"low\"]:\n            if sev in message_lower:\n                severities.append(sev)\n\n        return ExtractedEntities(\n            aws_services=aws_services,\n            compliance_frameworks=compliance,\n            resource_ids=[],  # Extracted later from context\n            severity_levels=severities,\n            finding_ids=finding_ids,\n            regions=[],\n            keywords=self._extract_keywords(message),\n        )\n\n    def _extract_keywords(self, message: str) -> list[str]:\n        \"\"\"Extract key terms from message.\"\"\"\n        # Simple keyword extraction - could be enhanced with NLP\n        stopwords = {\"the\", \"a\", \"an\", \"is\", \"are\", \"what\", \"how\", \"should\", \"i\", \"my\", \"me\"}\n        words = re.findall(r'\\b\\w+\\b', message.lower())\n        return [w for w in words if w not in stopwords and len(w) > 2]\n\n    def _build_context(\n        self,\n        history: list[dict],\n        entities: ExtractedEntities,\n    ) -> ConversationContext:\n        \"\"\"Build context from conversation history.\"\"\"\n        # Extract topic thread\n        topics = []\n        for msg in history[-5:]:  # Last 5 messages\n            if msg[\"role\"] == \"user\":\n                topics.extend(self._extract_keywords(msg[\"content\"]))\n\n        # Track mentioned resources\n        resources = []\n        for msg in history:\n            if \"resource\" in msg.get(\"metadata\", {}):\n                resources.append(msg[\"metadata\"][\"resource\"])\n\n        return ConversationContext(\n            topics=list(set(topics)),\n            mentioned_services=entities.aws_services,\n            mentioned_frameworks=entities.compliance_frameworks,\n            active_resources=resources,\n            turn_count=len([m for m in history if m[\"role\"] == \"user\"]),\n        )\n\n    def _requires_findings(self, intent: Intent) -> bool:\n        \"\"\"Check if intent requires findings data.\"\"\"\n        return intent in [\n            Intent.FINDING_EXPLANATION,\n            Intent.REMEDIATION_HELP,\n            Intent.SECURITY_ADVICE,\n            Intent.COMPLIANCE_QUESTION,\n        ]\n\n    def _requires_documents(\n        self,\n        intent: Intent,\n        history: list[dict],\n    ) -> bool:\n        \"\"\"Check if intent requires document context.\"\"\"\n        if intent == Intent.DOCUMENT_ANALYSIS:\n            return True\n\n        # Check if documents were uploaded in conversation\n        for msg in history:\n            if msg.get(\"metadata\", {}).get(\"documents\"):\n                return True\n\n        return False\n```\n\n### NLU Result Model\n\n```python\n# src/ib_platform/nlu/models.py\nfrom dataclasses import dataclass\n\n@dataclass\nclass NLUResult:\n    intent: Intent\n    confidence: float\n    entities: ExtractedEntities\n    context: ConversationContext\n    requires_findings: bool\n    requires_documents: bool\n\n    @property\n    def is_confident(self) -> bool:\n        return self.confidence >= 0.7\n\n    def to_dict(self) -> dict:\n        return {\n            \"intent\": self.intent.value,\n            \"confidence\": self.confidence,\n            \"entities\": {\n                \"aws_services\": self.entities.aws_services,\n                \"compliance_frameworks\": self.entities.compliance_frameworks,\n                \"finding_ids\": self.entities.finding_ids,\n            },\n            \"requires_findings\": self.requires_findings,\n            \"requires_documents\": self.requires_documents,\n        }\n\n\n@dataclass\nclass ConversationContext:\n    topics: list[str]\n    mentioned_services: list[str]\n    mentioned_frameworks: list[str]\n    active_resources: list[str]\n    turn_count: int\n```\n\n## API Integration\n\n```python\n# Used internally by chat service\nasync def handle_message(message: str, conversation_id: str):\n    # Get conversation history\n    history = await get_conversation_history(conversation_id)\n\n    # Process through NLU\n    nlu_result = await nlu_service.process(message, history)\n\n    # Route to appropriate handler based on intent\n    if nlu_result.intent == Intent.FINDING_EXPLANATION:\n        return await handle_finding_explanation(message, nlu_result)\n    elif nlu_result.intent == Intent.COMPLIANCE_QUESTION:\n        return await handle_compliance_question(message, nlu_result)\n    # ... other handlers\n```\n\n## Files to Create\n\n```\nsrc/ib_platform/nlu/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 intents.py               # Intent definitions and examples\n\u251c\u2500\u2500 entities.py              # Entity extraction\n\u251c\u2500\u2500 service.py               # Main NLU service\n\u251c\u2500\u2500 models.py                # NLU result models\n\u2514\u2500\u2500 context.py               # Conversation context tracking\n\ntests/ib_platform/nlu/\n\u251c\u2500\u2500 test_intent_classification.py\n\u251c\u2500\u2500 test_entity_extraction.py\n\u251c\u2500\u2500 test_context_building.py\n\u2514\u2500\u2500 test_nlu_service.py\n```\n\n## Testing Requirements\n\n### Unit Tests\n- [ ] `test_intent_classification.py` - Each intent classified correctly\n- [ ] `test_entity_extraction.py` - AWS services, frameworks extracted\n- [ ] `test_context_building.py` - History context built correctly\n- [ ] `test_keyword_extraction.py` - Keywords extracted\n\n### Integration Tests\n- [ ] `test_nlu_pipeline.py` - Full pipeline with mocked LLM\n- [ ] `test_multi_turn.py` - Multi-turn conversation context\n\n### Test Data\n\n```python\nTEST_CASES = [\n    {\n        \"message\": \"What security concerns should I have with patient data in S3 going through Glue to Redshift?\",\n        \"expected_intent\": Intent.SECURITY_ADVICE,\n        \"expected_services\": [\"S3\", \"GLUE\", \"REDSHIFT\"],\n        \"expected_frameworks\": [\"HIPAA\"],  # \"patient data\" implies HIPAA\n    },\n    {\n        \"message\": \"Explain finding S3_001\",\n        \"expected_intent\": Intent.FINDING_EXPLANATION,\n        \"expected_finding_ids\": [\"S3_001\"],\n    },\n    {\n        \"message\": \"What do I need for SOC 2 compliance?\",\n        \"expected_intent\": Intent.COMPLIANCE_QUESTION,\n        \"expected_frameworks\": [\"SOC2\"],\n    },\n]\n```\n\n## Acceptance Criteria Checklist\n\n- [ ] Intent classification >90% accuracy on test set\n- [ ] AWS services extracted from questions\n- [ ] Compliance frameworks detected from context\n- [ ] Finding IDs extracted (e.g., S3_001)\n- [ ] Conversation context tracks topics\n- [ ] Multi-turn context preserved\n- [ ] Low-confidence detection works\n- [ ] Out-of-scope questions handled gracefully\n- [ ] 80%+ test coverage\n\n## Dependencies\n\n- Anthropic API access (for Claude)\n\n## Blocked By\n\n- None (first IB component)\n\n## Blocks\n\n- 8.2 Answer Generation (uses NLU output)\n- 8.3 Security Analysis (uses intent/entities)\n\n## Estimated Effort\n\n1.5 weeks\n\n## Labels\n\n`nlu`, `ib`, `ai`, `mvp`, `phase-2`, `P0`\n",
  "labels": [
    "mvp",
    "phase-2",
    "P0",
    "ib",
    "ai",
    "nlu"
  ],
  "assignees": [],
  "milestone": null,
  "created_at": null,
  "updated_at": null,
  "state": "OPEN",
  "context": {
    "complexity": "medium",
    "effort": "medium",
    "confidence": 0.8,
    "required_expertise": [
      "database",
      "backend"
    ],
    "dependencies": [],
    "related_files": []
  },
  "generated_at": "2025-12-04T12:03:26.295540Z",
  "generated_by": "smart-scaffold-cli",
  "branch_name": "feature/issue-35-81nlupipeline"
}