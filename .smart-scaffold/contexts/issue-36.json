{
  "issue_number": 36,
  "title": "8.2 Answer Generation Engine",
  "body": "# 8.2 Answer Generation Engine\n\n## Parent Epic\nEpic 8: MVP Phase 2 - Expert System (Intelligence-Builder)\n\n## Overview\n\nImplement the answer generation engine that produces expert-level security responses using LLM (Claude) with context from the knowledge base, findings, and documents. Supports streaming responses for real-time UI feedback.\n\n## Background\n\nAnswer generation is the core value delivery mechanism. It must:\n- Provide expert-level security advice\n- Ground responses in compliance frameworks\n- Reference specific findings when relevant\n- Stream responses for better UX\n- Include actionable remediation steps\n\n## Requirements\n\n| ID | Requirement | Acceptance Criteria |\n|----|-------------|---------------------|\n| ANS-001 | Context assembly | Gather relevant KB entries, findings, documents for context |\n| ANS-002 | Expert prompting | Use security expert persona with compliance awareness |\n| ANS-003 | Streaming generation | Stream response chunks via SSE |\n| ANS-004 | Response structure | Include severity, compliance mapping, remediation |\n| ANS-005 | Citation handling | Reference sources (findings, KB, documents) |\n\n## Technical Specification\n\n### Answer Generation Architecture\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Answer Generation Pipeline                        \u2502\n\u2502                                                                       \u2502\n\u2502  NLU Result \u2192 Context Assembly \u2192 Prompt Building \u2192 LLM \u2192 Streaming   \u2502\n\u2502                                                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502                    Context Sources                               \u2502\u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\u2502\n\u2502  \u2502  \u2502 Compliance   \u2502  \u2502 Findings     \u2502  \u2502 Documents            \u2502  \u2502\u2502\n\u2502  \u2502  \u2502 KB           \u2502  \u2502 (if any)     \u2502  \u2502 (if uploaded)        \u2502  \u2502\u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Answer Service\n\n```python\n# src/ib_platform/answer/service.py\nfrom anthropic import Anthropic\nfrom typing import AsyncIterator\n\nclass AnswerService:\n    SYSTEM_PROMPT = \"\"\"You are a cloud security expert assistant for Cloud Optimizer.\nYour role is to provide accurate, actionable security advice for AWS environments.\n\nKey behaviors:\n1. Ground advice in specific compliance frameworks (HIPAA, SOC2, PCI-DSS, GDPR, CIS, NIST)\n2. Provide specific, actionable remediation steps\n3. Include severity assessment when discussing risks\n4. Reference the user's actual findings when available\n5. Be concise but thorough - prioritize the most important points\n6. Use markdown formatting for readability\n7. If you're not sure about something, say so\n\nWhen discussing findings:\n- Always mention the compliance frameworks affected\n- Provide the specific remediation steps\n- Include code snippets (Terraform, CLI, Console steps) when helpful\n\nFormat severity indicators:\n- \ud83d\udd34 CRITICAL: Immediate action required\n- \ud83d\udfe0 HIGH: Address within 24-48 hours\n- \ud83d\udfe1 MEDIUM: Address within 1-2 weeks\n- \ud83d\udfe2 LOW: Address in next review cycle\"\"\"\n\n    def __init__(\n        self,\n        anthropic_client: Anthropic,\n        kb_service: KnowledgeBaseService,\n        findings_service: FindingsService,\n        document_service: DocumentService,\n    ):\n        self.client = anthropic_client\n        self.kb_service = kb_service\n        self.findings_service = findings_service\n        self.document_service = document_service\n\n    async def generate(\n        self,\n        question: str,\n        nlu_result: NLUResult,\n        tenant_id: UUID,\n        conversation_history: list[dict],\n    ) -> AsyncIterator[str]:\n        \"\"\"Generate answer with streaming.\"\"\"\n        # Assemble context\n        context = await self._assemble_context(nlu_result, tenant_id)\n\n        # Build messages\n        messages = self._build_messages(\n            question, context, conversation_history\n        )\n\n        # Stream response\n        async with self.client.messages.stream(\n            model=\"claude-3-5-sonnet-20241022\",\n            max_tokens=2000,\n            system=self.SYSTEM_PROMPT,\n            messages=messages,\n        ) as stream:\n            async for text in stream.text_stream:\n                yield text\n\n    async def _assemble_context(\n        self,\n        nlu_result: NLUResult,\n        tenant_id: UUID,\n    ) -> AnswerContext:\n        \"\"\"Assemble context from various sources.\"\"\"\n        context = AnswerContext()\n\n        # Get relevant KB entries\n        if nlu_result.entities.compliance_frameworks:\n            for framework in nlu_result.entities.compliance_frameworks:\n                kb_entries = await self.kb_service.get_for_framework(framework)\n                context.kb_entries.extend(kb_entries)\n\n        if nlu_result.entities.aws_services:\n            for service in nlu_result.entities.aws_services:\n                kb_entries = await self.kb_service.get_for_service(service)\n                context.kb_entries.extend(kb_entries)\n\n        # Get relevant findings\n        if nlu_result.requires_findings:\n            findings = await self._get_relevant_findings(nlu_result, tenant_id)\n            context.findings = findings\n\n        # Get document context\n        if nlu_result.requires_documents:\n            docs = await self.document_service.get_for_conversation(\n                tenant_id, nlu_result.context\n            )\n            context.documents = docs\n\n        return context\n\n    async def _get_relevant_findings(\n        self,\n        nlu_result: NLUResult,\n        tenant_id: UUID,\n    ) -> list[Finding]:\n        \"\"\"Get findings relevant to the question.\"\"\"\n        # If specific finding IDs mentioned, get those\n        if nlu_result.entities.finding_ids:\n            return await self.findings_service.get_by_rule_ids(\n                tenant_id, nlu_result.entities.finding_ids\n            )\n\n        # Otherwise, get findings for mentioned services\n        filters = FindingFilters()\n        if nlu_result.entities.aws_services:\n            filters.resource_type = [\n                f\"{svc.lower()}_*\" for svc in nlu_result.entities.aws_services\n            ]\n        if nlu_result.entities.compliance_frameworks:\n            filters.compliance_framework = nlu_result.entities.compliance_frameworks[0]\n\n        page = await self.findings_service.get_findings(\n            tenant_id, filters, Pagination(limit=10)\n        )\n        return page.items\n\n    def _build_messages(\n        self,\n        question: str,\n        context: AnswerContext,\n        history: list[dict],\n    ) -> list[dict]:\n        \"\"\"Build messages array for Claude.\"\"\"\n        messages = []\n\n        # Add conversation history (last 10 messages)\n        for msg in history[-10:]:\n            messages.append({\n                \"role\": msg[\"role\"],\n                \"content\": msg[\"content\"],\n            })\n\n        # Build user message with context\n        user_content = self._build_user_message(question, context)\n        messages.append({\"role\": \"user\", \"content\": user_content})\n\n        return messages\n\n    def _build_user_message(\n        self,\n        question: str,\n        context: AnswerContext,\n    ) -> str:\n        \"\"\"Build enriched user message with context.\"\"\"\n        parts = []\n\n        # Add findings context\n        if context.findings:\n            parts.append(\"## Current Security Findings\\n\")\n            for finding in context.findings[:5]:  # Top 5\n                parts.append(f\"- **{finding.title}** ({finding.severity})\")\n                parts.append(f\"  Resource: {finding.resource_id}\")\n                parts.append(f\"  Compliance: {', '.join(finding.compliance_frameworks)}\")\n            parts.append(\"\")\n\n        # Add compliance context\n        if context.kb_entries:\n            parts.append(\"## Relevant Compliance Requirements\\n\")\n            for entry in context.kb_entries[:5]:\n                parts.append(f\"- **{entry.framework}**: {entry.control_name}\")\n                parts.append(f\"  {entry.description[:200]}...\")\n            parts.append(\"\")\n\n        # Add document context\n        if context.documents:\n            parts.append(\"## Architecture Context\\n\")\n            for doc in context.documents:\n                parts.append(f\"From document '{doc.filename}':\")\n                parts.append(doc.summary[:500])\n            parts.append(\"\")\n\n        # Add the actual question\n        parts.append(\"## User Question\\n\")\n        parts.append(question)\n\n        return \"\\n\".join(parts)\n\n\n@dataclass\nclass AnswerContext:\n    kb_entries: list[KBEntry] = field(default_factory=list)\n    findings: list[Finding] = field(default_factory=list)\n    documents: list[DocumentContext] = field(default_factory=list)\n```\n\n### Streaming Handler\n\n```python\n# src/ib_platform/answer/streaming.py\nimport asyncio\nfrom typing import AsyncIterator\nimport json\n\nclass StreamingHandler:\n    \"\"\"Handle SSE streaming of answer chunks.\"\"\"\n\n    def __init__(self, answer_service: AnswerService):\n        self.answer_service = answer_service\n\n    async def stream_answer(\n        self,\n        question: str,\n        nlu_result: NLUResult,\n        tenant_id: UUID,\n        conversation_history: list[dict],\n    ) -> AsyncIterator[str]:\n        \"\"\"Generate SSE events for streaming response.\"\"\"\n        try:\n            # Send start event\n            yield self._sse_event(\"start\", {\"type\": \"start\"})\n\n            # Stream answer chunks\n            full_response = \"\"\n            async for chunk in self.answer_service.generate(\n                question, nlu_result, tenant_id, conversation_history\n            ):\n                full_response += chunk\n                yield self._sse_event(\"chunk\", {\"content\": chunk})\n\n            # Send completion event with metadata\n            yield self._sse_event(\"done\", {\n                \"type\": \"done\",\n                \"intent\": nlu_result.intent.value,\n                \"entities\": nlu_result.entities.aws_services,\n            })\n\n        except Exception as e:\n            yield self._sse_event(\"error\", {\n                \"type\": \"error\",\n                \"message\": str(e),\n            })\n\n    def _sse_event(self, event: str, data: dict) -> str:\n        \"\"\"Format as SSE event.\"\"\"\n        return f\"event: {event}\\ndata: {json.dumps(data)}\\n\\n\"\n\n\n# FastAPI endpoint\n@router.post(\"/chat/stream\")\nasync def stream_chat(\n    request: ChatRequest,\n    tenant_id: UUID = Depends(get_tenant_id),\n):\n    async def generate():\n        # Process through NLU\n        nlu_result = await nlu_service.process(\n            request.message, request.conversation_history\n        )\n\n        # Stream answer\n        async for event in streaming_handler.stream_answer(\n            request.message,\n            nlu_result,\n            tenant_id,\n            request.conversation_history,\n        ):\n            yield event\n\n    return StreamingResponse(\n        generate(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n        },\n    )\n```\n\n### Response Formatter\n\n```python\n# src/ib_platform/answer/formatter.py\nclass ResponseFormatter:\n    \"\"\"Format responses with consistent structure.\"\"\"\n\n    @staticmethod\n    def format_security_advice(\n        content: str,\n        findings: list[Finding] = None,\n        compliance: list[str] = None,\n    ) -> str:\n        \"\"\"Format security advice response.\"\"\"\n        parts = [content]\n\n        if findings:\n            parts.append(\"\\n---\\n\")\n            parts.append(\"**Related Findings:**\\n\")\n            for f in findings[:3]:\n                parts.append(f\"- [{f.rule_id}] {f.title} ({f.severity})\")\n\n        if compliance:\n            parts.append(\"\\n**Compliance Frameworks:**\")\n            parts.append(\", \".join(compliance))\n\n        return \"\\n\".join(parts)\n\n    @staticmethod\n    def format_remediation(\n        title: str,\n        steps: list[str],\n        code: str = None,\n        language: str = \"hcl\",\n    ) -> str:\n        \"\"\"Format remediation guidance.\"\"\"\n        parts = [f\"## Remediation: {title}\\n\"]\n\n        parts.append(\"### Steps:\\n\")\n        for i, step in enumerate(steps, 1):\n            parts.append(f\"{i}. {step}\")\n\n        if code:\n            parts.append(f\"\\n### Code Example ({language}):\\n\")\n            parts.append(f\"```{language}\")\n            parts.append(code)\n            parts.append(\"```\")\n\n        return \"\\n\".join(parts)\n```\n\n## API Endpoints\n\n```\nPOST /api/v1/chat/message           # Send message (returns full response)\nPOST /api/v1/chat/stream            # Stream response via SSE\nGET  /api/v1/chat/stream/:id        # Connect to existing stream\n```\n\n## Files to Create\n\n```\nsrc/ib_platform/answer/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 service.py               # Main answer service\n\u251c\u2500\u2500 streaming.py             # SSE streaming handler\n\u251c\u2500\u2500 context.py               # Context assembly\n\u251c\u2500\u2500 formatter.py             # Response formatting\n\u2514\u2500\u2500 prompts.py               # System prompts\n\ntests/ib_platform/answer/\n\u251c\u2500\u2500 test_context_assembly.py\n\u251c\u2500\u2500 test_answer_generation.py\n\u251c\u2500\u2500 test_streaming.py\n\u2514\u2500\u2500 test_formatter.py\n```\n\n## Testing Requirements\n\n### Unit Tests\n- [ ] `test_context_assembly.py` - Context gathered correctly\n- [ ] `test_prompt_building.py` - Messages built correctly\n- [ ] `test_formatter.py` - Response formatting\n\n### Integration Tests\n- [ ] `test_answer_generation.py` - Full generation with mocked LLM\n- [ ] `test_streaming.py` - SSE streaming works\n\n### Test Mocking\n\n```python\n@pytest.fixture\ndef mock_anthropic():\n    with patch(\"anthropic.Anthropic\") as mock:\n        client = MagicMock()\n\n        # Mock streaming response\n        async def mock_stream():\n            for chunk in [\"Here's \", \"my \", \"response.\"]:\n                yield chunk\n\n        client.messages.stream.return_value.__aenter__.return_value.text_stream = mock_stream()\n        mock.return_value = client\n        yield client\n```\n\n## Acceptance Criteria Checklist\n\n- [ ] Context assembled from KB, findings, documents\n- [ ] Expert-level prompting produces quality responses\n- [ ] Responses include compliance framework references\n- [ ] Severity indicators used appropriately\n- [ ] Remediation steps included for findings\n- [ ] SSE streaming works smoothly\n- [ ] Conversation history maintained\n- [ ] Response time <3 seconds to first chunk\n- [ ] 80%+ test coverage\n\n## Dependencies\n\n- 8.1 NLU Pipeline (provides NLU result)\n- 8.5 Knowledge Base (provides KB context)\n- 7.4 Findings Management (provides findings)\n\n## Blocked By\n\n- 8.1 NLU Pipeline\n\n## Blocks\n\n- 6.5 Chat Interface (uses streaming)\n\n## Estimated Effort\n\n1.5 weeks\n\n## Labels\n\n`answer-generation`, `ib`, `ai`, `streaming`, `mvp`, `phase-2`, `P0`\n",
  "labels": [
    "mvp",
    "phase-2",
    "P0",
    "ib",
    "ai",
    "streaming",
    "answer-generation"
  ],
  "assignees": [],
  "milestone": null,
  "created_at": null,
  "updated_at": null,
  "state": "OPEN",
  "context": {
    "complexity": "medium",
    "effort": "medium",
    "confidence": 0.8,
    "required_expertise": [
      "backend"
    ],
    "dependencies": [],
    "related_files": []
  },
  "generated_at": "2025-12-04T12:04:13.775636Z",
  "generated_by": "smart-scaffold-cli",
  "branch_name": "feature/issue-36-82answergenerationengine"
}