{
  "issue_number": 38,
  "title": "8.4 Document Analysis Service",
  "body": "# 8.4 Document Analysis Service\n\n## Parent Epic\nEpic 8: MVP Phase 2 - Expert System (Intelligence-Builder)\n\n## Overview\n\nImplement document analysis capabilities that process uploaded architecture documents (PDF, TXT) to extract entities, identify security concerns, and provide recommendations. Enables the \"upload your architecture and get recommendations\" use case.\n\n## Background\n\nDocument analysis is a key differentiator for trial customers who want to:\n- Get security review of architecture designs\n- Understand compliance implications of proposed architectures\n- Identify missing security controls before deployment\n- Get recommendations for security improvements\n\n## Requirements\n\n| ID | Requirement | Acceptance Criteria |\n|----|-------------|---------------------|\n| DOC-001 | PDF parsing | Extract text from PDF documents, handle multi-page |\n| DOC-002 | Entity extraction | Identify AWS services, data flows, security controls |\n| DOC-003 | Security analysis | Identify security gaps and recommend improvements |\n| DOC-004 | Document context | Use document content in chat responses |\n\n## Technical Specification\n\n### Document Processing Architecture\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Document Processing Pipeline                      \u2502\n\u2502                                                                       \u2502\n\u2502  Upload \u2192 Extraction \u2192 Entity Analysis \u2192 Security Review \u2192 Storage   \u2502\n\u2502                                                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502  PDF/TXT \u2192 Text Extraction \u2192 LLM Analysis \u2192 Structured Output   \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Database Schema\n\n```sql\n-- Uploaded documents\nCREATE TABLE documents (\n    document_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    tenant_id UUID NOT NULL REFERENCES tenants(tenant_id),\n    conversation_id UUID,  -- Associated chat conversation\n    filename VARCHAR(255) NOT NULL,\n    content_type VARCHAR(100) NOT NULL,\n    file_size INTEGER NOT NULL,\n    storage_path VARCHAR(500) NOT NULL,  -- S3 or local path\n    status VARCHAR(20) NOT NULL DEFAULT 'processing',  -- processing, analyzed, error\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\n-- Document analysis results\nCREATE TABLE document_analyses (\n    analysis_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    document_id UUID NOT NULL REFERENCES documents(document_id),\n    extracted_text TEXT,\n    summary TEXT,\n    entities JSONB,          -- AWS services, data flows, etc.\n    security_concerns JSONB, -- Identified issues\n    recommendations JSONB,   -- Suggested improvements\n    compliance_gaps JSONB,   -- Missing compliance controls\n    analyzed_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\nCREATE INDEX idx_documents_tenant ON documents(tenant_id);\nCREATE INDEX idx_documents_conversation ON documents(conversation_id);\nCREATE INDEX idx_analyses_document ON document_analyses(document_id);\n```\n\n### Document Service\n\n```python\n# src/ib_platform/document/service.py\nimport pypdf\nfrom anthropic import Anthropic\nfrom pathlib import Path\n\nclass DocumentService:\n    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\n    ALLOWED_TYPES = [\"application/pdf\", \"text/plain\"]\n\n    def __init__(\n        self,\n        db: AsyncSession,\n        anthropic_client: Anthropic,\n        storage_path: Path,\n    ):\n        self.db = db\n        self.client = anthropic_client\n        self.storage_path = storage_path\n\n    async def upload(\n        self,\n        tenant_id: UUID,\n        file: UploadFile,\n        conversation_id: UUID = None,\n    ) -> Document:\n        \"\"\"Upload and process a document.\"\"\"\n        # Validate file\n        if file.content_type not in self.ALLOWED_TYPES:\n            raise InvalidDocumentTypeException(\n                f\"Unsupported file type: {file.content_type}\"\n            )\n\n        content = await file.read()\n        if len(content) > self.MAX_FILE_SIZE:\n            raise DocumentTooLargeException(\n                f\"File exceeds {self.MAX_FILE_SIZE / 1024 / 1024}MB limit\"\n            )\n\n        # Save file\n        storage_path = await self._save_file(tenant_id, file.filename, content)\n\n        # Create document record\n        document = Document(\n            tenant_id=tenant_id,\n            conversation_id=conversation_id,\n            filename=file.filename,\n            content_type=file.content_type,\n            file_size=len(content),\n            storage_path=str(storage_path),\n            status=\"processing\",\n        )\n        self.db.add(document)\n        await self.db.commit()\n\n        # Start async analysis\n        asyncio.create_task(self._analyze_document(document.document_id))\n\n        return document\n\n    async def _analyze_document(self, document_id: UUID):\n        \"\"\"Analyze document content.\"\"\"\n        document = await self._get_document(document_id)\n\n        try:\n            # Extract text\n            text = await self._extract_text(document)\n\n            # Analyze with LLM\n            analysis = await self._analyze_with_llm(text)\n\n            # Save analysis\n            doc_analysis = DocumentAnalysis(\n                document_id=document_id,\n                extracted_text=text[:50000],  # Limit stored text\n                summary=analysis[\"summary\"],\n                entities=analysis[\"entities\"],\n                security_concerns=analysis[\"security_concerns\"],\n                recommendations=analysis[\"recommendations\"],\n                compliance_gaps=analysis[\"compliance_gaps\"],\n            )\n            self.db.add(doc_analysis)\n\n            document.status = \"analyzed\"\n            await self.db.commit()\n\n        except Exception as e:\n            document.status = \"error\"\n            await self.db.commit()\n            raise\n\n    async def _extract_text(self, document: Document) -> str:\n        \"\"\"Extract text from document.\"\"\"\n        file_path = Path(document.storage_path)\n\n        if document.content_type == \"application/pdf\":\n            return await self._extract_pdf_text(file_path)\n        elif document.content_type == \"text/plain\":\n            return file_path.read_text()\n        else:\n            raise UnsupportedDocumentTypeException()\n\n    async def _extract_pdf_text(self, file_path: Path) -> str:\n        \"\"\"Extract text from PDF.\"\"\"\n        text_parts = []\n\n        with open(file_path, \"rb\") as f:\n            reader = pypdf.PdfReader(f)\n            for page in reader.pages:\n                text = page.extract_text()\n                if text:\n                    text_parts.append(text)\n\n        return \"\\n\\n\".join(text_parts)\n\n    async def _analyze_with_llm(self, text: str) -> dict:\n        \"\"\"Analyze document text with LLM.\"\"\"\n        prompt = f\"\"\"Analyze this architecture document for AWS security.\n\nDocument content:\n---\n{text[:30000]}\n---\n\nProvide analysis in JSON format:\n{{\n    \"summary\": \"Brief summary of the architecture (2-3 sentences)\",\n    \"entities\": {{\n        \"aws_services\": [\"list of AWS services mentioned\"],\n        \"data_flows\": [\"description of data flows\"],\n        \"data_types\": [\"types of data handled (PII, PHI, financial, etc.)\"],\n        \"external_integrations\": [\"third-party services/APIs\"]\n    }},\n    \"security_concerns\": [\n        {{\n            \"title\": \"Concern title\",\n            \"severity\": \"high/medium/low\",\n            \"description\": \"Description of the concern\",\n            \"affected_components\": [\"components affected\"]\n        }}\n    ],\n    \"recommendations\": [\n        {{\n            \"title\": \"Recommendation title\",\n            \"priority\": \"high/medium/low\",\n            \"description\": \"What to do\",\n            \"compliance_relevance\": [\"HIPAA\", \"SOC2\", etc.]\n        }}\n    ],\n    \"compliance_gaps\": [\n        {{\n            \"framework\": \"HIPAA/SOC2/PCI-DSS/etc.\",\n            \"gap\": \"Description of what's missing\",\n            \"recommendation\": \"How to address it\"\n        }}\n    ]\n}}\"\"\"\n\n        response = await self.client.messages.create(\n            model=\"claude-3-5-sonnet-20241022\",\n            max_tokens=3000,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n\n        # Parse JSON response\n        content = response.content[0].text\n        # Extract JSON from response (may be wrapped in markdown)\n        json_match = re.search(r'\\{[\\s\\S]*\\}', content)\n        if json_match:\n            return json.loads(json_match.group())\n\n        raise AnalysisParsingException(\"Could not parse analysis response\")\n\n    async def get_document(\n        self,\n        tenant_id: UUID,\n        document_id: UUID,\n    ) -> Document:\n        \"\"\"Get document by ID.\"\"\"\n        result = await self.db.execute(\n            select(Document)\n            .where(Document.document_id == document_id)\n            .where(Document.tenant_id == tenant_id)\n        )\n        document = result.scalar_one_or_none()\n        if not document:\n            raise DocumentNotFoundException()\n        return document\n\n    async def get_analysis(\n        self,\n        tenant_id: UUID,\n        document_id: UUID,\n    ) -> DocumentAnalysis:\n        \"\"\"Get document analysis.\"\"\"\n        document = await self.get_document(tenant_id, document_id)\n\n        result = await self.db.execute(\n            select(DocumentAnalysis)\n            .where(DocumentAnalysis.document_id == document_id)\n        )\n        analysis = result.scalar_one_or_none()\n        if not analysis:\n            raise AnalysisNotFoundException()\n        return analysis\n\n    async def get_for_conversation(\n        self,\n        tenant_id: UUID,\n        conversation_id: UUID,\n    ) -> list[DocumentContext]:\n        \"\"\"Get document context for a conversation.\"\"\"\n        result = await self.db.execute(\n            select(Document, DocumentAnalysis)\n            .join(DocumentAnalysis)\n            .where(Document.tenant_id == tenant_id)\n            .where(Document.conversation_id == conversation_id)\n            .where(Document.status == \"analyzed\")\n        )\n\n        contexts = []\n        for doc, analysis in result:\n            contexts.append(\n                DocumentContext(\n                    document_id=doc.document_id,\n                    filename=doc.filename,\n                    summary=analysis.summary,\n                    entities=analysis.entities,\n                    security_concerns=analysis.security_concerns,\n                )\n            )\n\n        return contexts\n\n    async def delete_document(\n        self,\n        tenant_id: UUID,\n        document_id: UUID,\n    ):\n        \"\"\"Delete document and analysis.\"\"\"\n        document = await self.get_document(tenant_id, document_id)\n\n        # Delete file\n        file_path = Path(document.storage_path)\n        if file_path.exists():\n            file_path.unlink()\n\n        # Delete records\n        await self.db.execute(\n            delete(DocumentAnalysis)\n            .where(DocumentAnalysis.document_id == document_id)\n        )\n        await self.db.execute(\n            delete(Document)\n            .where(Document.document_id == document_id)\n        )\n        await self.db.commit()\n```\n\n### Document Context for Chat\n\n```python\n# src/ib_platform/document/context.py\n@dataclass\nclass DocumentContext:\n    document_id: UUID\n    filename: str\n    summary: str\n    entities: dict\n    security_concerns: list[dict]\n\n    def to_prompt_context(self) -> str:\n        \"\"\"Format for inclusion in chat prompts.\"\"\"\n        lines = [\n            f\"**Document: {self.filename}**\",\n            f\"Summary: {self.summary}\",\n            \"\",\n            \"**AWS Services:**\",\n            \", \".join(self.entities.get(\"aws_services\", [])),\n            \"\",\n            \"**Data Types:**\",\n            \", \".join(self.entities.get(\"data_types\", [])),\n            \"\",\n            \"**Security Concerns:**\",\n        ]\n\n        for concern in self.security_concerns[:5]:\n            lines.append(f\"- [{concern['severity']}] {concern['title']}\")\n\n        return \"\\n\".join(lines)\n```\n\n## API Endpoints\n\n```\nPOST /api/v1/documents/upload        # Upload document\nGET  /api/v1/documents               # List documents\nGET  /api/v1/documents/:id           # Get document\nGET  /api/v1/documents/:id/analysis  # Get analysis\nDELETE /api/v1/documents/:id         # Delete document\nGET  /api/v1/documents/:id/download  # Download original\n```\n\n## Files to Create\n\n```\nsrc/ib_platform/document/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 service.py               # Main document service\n\u251c\u2500\u2500 extraction.py            # Text extraction\n\u251c\u2500\u2500 analysis.py              # LLM analysis\n\u251c\u2500\u2500 context.py               # Chat context\n\u2514\u2500\u2500 models.py                # Data models\n\nsrc/cloud_optimizer/api/routers/\n\u2514\u2500\u2500 documents.py             # API endpoints\n\nalembic/versions/\n\u2514\u2500\u2500 xxx_create_document_tables.py\n\ntests/ib_platform/document/\n\u251c\u2500\u2500 test_extraction.py\n\u251c\u2500\u2500 test_analysis.py\n\u251c\u2500\u2500 test_service.py\n\u2514\u2500\u2500 test_api.py\n```\n\n## Testing Requirements\n\n### Unit Tests\n- [ ] `test_pdf_extraction.py` - PDF text extraction\n- [ ] `test_txt_extraction.py` - TXT file handling\n- [ ] `test_entity_extraction.py` - Entity parsing from analysis\n- [ ] `test_context_formatting.py` - Context for chat\n\n### Integration Tests\n- [ ] `test_document_upload.py` - Full upload flow\n- [ ] `test_document_analysis.py` - Analysis with mocked LLM\n\n### Test Files\n\n```\ntests/fixtures/documents/\n\u251c\u2500\u2500 sample_architecture.pdf\n\u251c\u2500\u2500 simple_design.txt\n\u2514\u2500\u2500 complex_multi_page.pdf\n```\n\n## Acceptance Criteria Checklist\n\n- [ ] PDF upload works (up to 10MB)\n- [ ] TXT upload works\n- [ ] Text extracted from multi-page PDFs\n- [ ] AWS services identified from documents\n- [ ] Data flows extracted\n- [ ] Security concerns identified with severity\n- [ ] Recommendations provided with priority\n- [ ] Compliance gaps identified\n- [ ] Document context available for chat\n- [ ] Trial limit enforced for documents\n- [ ] 80%+ test coverage\n\n## Dependencies\n\n- 6.3 Trial Management (document limit)\n- 6.5 Chat Interface (document upload UI)\n\n## Blocked By\n\n- 6.5 Chat Interface (upload capability)\n\n## Blocks\n\n- 8.2 Answer Generation (document context)\n\n## Estimated Effort\n\n1.5 weeks\n\n## Labels\n\n`document`, `analysis`, `ib`, `pdf`, `mvp`, `phase-2`, `P0`\n",
  "labels": [
    "mvp",
    "phase-2",
    "P0",
    "ib",
    "analysis",
    "document",
    "pdf"
  ],
  "assignees": [],
  "milestone": null,
  "created_at": null,
  "updated_at": null,
  "state": "OPEN",
  "context": {
    "complexity": "medium",
    "effort": "medium",
    "confidence": 0.8,
    "required_expertise": [
      "database",
      "backend"
    ],
    "dependencies": [],
    "related_files": []
  },
  "generated_at": "2025-12-04T12:05:51.913809Z",
  "generated_by": "smart-scaffold-cli",
  "branch_name": "feature/issue-38-84documentanalysisservice"
}